# php如何使用较低内存读取2G的文件


1. 传统方式：在内存中读取文件内容

读取文件行的标准方式是在内存中读取.先将整个文件load进入内存.然后从内容中读取.

传统方式的问题： 是文件的所有行都被存放在内存中，当文件足够大时很快就会导致程序抛出`内存溢出`异常.


2. 分治法分批读取文件内容到内存,迭代处理法


问题思考：我们通常不需要把文件的所有行一次性地放入内存中，相反，我们只需要遍历文件的每一行，然后做相应的处理，
处理完之后把它扔掉。所以我们可 以通过行迭代方式来读取，而不是把所有行都放在内存中。


	<?php

	$fp = fopen("$sourceFile", "rb");
	fseek($fp, $range);

	set_time_limit(0);
	while (!feof($fp)) {

	    print (fread($fp, 1024 * 8)); //输出文件
	    flush();
	    ob_flush();

	}

	fclose($fp);
	exit ();

以流的形式读取文档内容.

## 使用php生成一个2G大小的execel

最近接到一个需求，通过选择的时间段导出对应的用户访问日志到excel中， 由于用户量较大，经常会有导出50万加数据的情况。
而常用的PHPexcel包需要把所有数据拿到后才能生成excel， 在面对生成超大数据量的excel文件时这显然是会造成内存溢出的，所以考虑使用让PHP边写入输出流边让浏览器下载的形式来完成需求。

我们通过如下的方式写入PHP输出流

	$fp = fopen('php://output', 'a');

	fputs($fp, 'strings');

	....

	....

	fclose($fp)


以下为原文作者思路：

`php://output`是一个可写的输出流，允许程序像操作文件一样将输出写入到输出流中，PHP会把输出流中的内容发送给web服务器并返回给发起请求的浏览器.

另外由于`excel`数据是从数据库里逐步读出然后写入输出流的所以需要将PHP的执行时间设长一点（默认30秒）`set_time_limit(0)`不对PHP执行时间做限制。

由于是逐步写入的无法获取文件的总体size所以就没办法通过设置header("Content-Length: $size");在下载前告诉浏览器这个文件有多大了。

逐步写入EXCEL的数据实际上来自Mysql的分页查询，随着offset越来越大Mysql在每次分页查询时需要跳过的行数就越多，这会严重影响Mysql查询的效率,所以采用LastId的方式来做分页查询.

-----有点实战经验的人都明白,按照这个方案一个1G的excel就能拖死nginx.虽然方案行不通，但是也有可取之处.我们可以通过异步的方式在服务端通过写流的方式生成超大excel.excel生成后通知前端进行断点下载.实际上整个流程对用户都是透明的,他只需要等待而已.



